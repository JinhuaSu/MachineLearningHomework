{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression homeweek in week 11\n",
    "\n",
    "**info**\n",
    "\n",
    "- SuJinhua\n",
    "- 2017201620\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('table5-6.xlsx')\n",
    "df = df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import f_regression\n",
    "from scipy import stats\n",
    "class gradual_regression(object):\n",
    "    def __init__(self,df,t_test_reject_value):\n",
    "        self.df = df\n",
    "        self.reject = t_test_reject_value\n",
    "    def _data(self,l):\n",
    "        return self.df[l].to_numpy()\n",
    "    def _fail_idx(self,p):\n",
    "        for i in range(len(p)-1,-1,-1):\n",
    "            if p[i] >= self.reject:\n",
    "                yield i\n",
    "    def gradual_reg(self):\n",
    "        select_set = list(self.df.columns)\n",
    "        y = self._data(select_set.pop())\n",
    "        init_set = []\n",
    "        garbage_set = []\n",
    "        count = 0\n",
    "        print('No.',count,init_set)\n",
    "        while len(select_set) > 0:\n",
    "            pop = select_set.pop()\n",
    "            init_set.append(pop)\n",
    "            count += 1\n",
    "            print('No.',count,init_set)\n",
    "            beta,t,F,p = self._regression(self._data(init_set),y)\n",
    "            flag = 1 \n",
    "            for i in self._fail_idx(p):\n",
    "                flag = 0\n",
    "                temp = init_set.pop(i)\n",
    "                count += 1\n",
    "                print('No.',count,init_set)\n",
    "                garbage_set.append(temp)\n",
    "                if pop == temp:\n",
    "                    break\n",
    "                flag = 1\n",
    "            if flag == 1:\n",
    "                select_set += garbage_set\n",
    "                garbage_set = []\n",
    "        beta,t,F,p = self._regression(self._data(init_set),y)\n",
    "        return self.pretty_report(init_set,beta,t,F,p,'gradual')\n",
    "    def backward_reg(self):\n",
    "        init_set = list(self.df.columns)\n",
    "        y = self._data(init_set.pop())\n",
    "        count = 0\n",
    "        print('No.',count,init_set)\n",
    "        while True:\n",
    "            beta,t,F,p = self._regression(self._data(init_set),y)\n",
    "            flag = 0\n",
    "            for i in self._fail_idx(p):\n",
    "                init_set.pop(i)\n",
    "                flag = 1\n",
    "                break\n",
    "            if flag == 1:\n",
    "                count += 1\n",
    "                print('No.',count,init_set)\n",
    "                continue\n",
    "            beta,t,F,p = self._regression(self._data(init_set),y)\n",
    "            return self.pretty_report(init_set,beta,t,F,p,'backward')\n",
    "    def pretty_report(self,init_set,beta,t,F,p,method):\n",
    "        def linear(init_set,beta,t,F,p):\n",
    "            init_set.insert(0,'cons_')\n",
    "            F = list(F)\n",
    "            p = list(p)\n",
    "            F.insert(0,0)\n",
    "            p.insert(0,0)\n",
    "            s = ''\n",
    "            for i in range(len(init_set)):\n",
    "                s += ' | '.join([init_set[i],'%.2f' % beta[i],'%.2f' % t[i],'%.2f' % F[i],'%.2f' % p[i]]) + '\\n'\n",
    "            return s\n",
    "        s = \\\n",
    "\"\"\"\n",
    "regression result\\n\n",
    "method: {method}\\n\n",
    "-----------------\\n\n",
    "y | beta | t | F_i | p(F)\\n\n",
    "{linear_com}\n",
    "\"\"\"\n",
    "        return s.format(method = method,linear_com = linear(init_set,beta,t,F,p))\n",
    "    def _regression(self,x,y):\n",
    "        F_ = []\n",
    "        for i in range(x.shape[1]):\n",
    "            if i != x.shape[1] - 1:\n",
    "                x_ = np.hstack((np.ones((x.shape[0],1)),x[:,:i],x[:,i+1:]))\n",
    "            else:\n",
    "                x_ = np.hstack((np.ones((x.shape[0],1)),x[:,:i]))\n",
    "            beta = np.linalg.inv(x_.T.dot(x_)).dot(x_.T.dot(y))\n",
    "            SSE = np.sum((y - x_.dot(beta))**2)\n",
    "            F_.append(SSE)\n",
    "        x = np.hstack((np.ones((x.shape[0],1)),x))\n",
    "        beta = np.linalg.inv(x.T.dot(x)).dot(x.T.dot(y))\n",
    "        SSE = np.sum((y - x.dot(beta))**2)\n",
    "        SST = np.sum((y - y.mean())**2)\n",
    "        F_ = np.array(F_)\n",
    "        F_ = (F_ - SSE) / SSE * (x.shape[0] - x.shape[1])\n",
    "        p = (stats.f.sf(F_,1,x.shape[0]-x.shape[1]))\n",
    "        sigma2 = SSE / (x.shape[0]-x.shape[1])\n",
    "        c = np.linalg.inv(x.T.dot(x)).diagonal()\n",
    "        t = beta / (c*sigma2)**0.5\n",
    "        return beta,t, F_,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gradual_regression(df,0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Regression y on x2-x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "regression result\n",
      "\n",
      "method: OLS\n",
      "\n",
      "-----------------\n",
      "\n",
      "y | beta | t | F_i | p(F)\n",
      "\n",
      "cons_ | 5922.83 | 2.37 | 0.00 | 0.00\n",
      "x2 | 4.86 | 1.94 | 3.76 | 0.08\n",
      "x3 | 2.37 | 2.82 | 7.94 | 0.02\n",
      "x4 | -817.90 | -4.37 | 19.07 | 0.00\n",
      "x5 | 14.54 | 0.10 | 0.01 | 0.92\n",
      "x6 | -846.87 | -2.90 | 8.43 | 0.02\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init_set = list(df.keys())\n",
    "y_str = init_set.pop()\n",
    "beta,t,F,p = model._regression(model._data(init_set),model._data(y_str))\n",
    "print(model.pretty_report(init_set,beta,t,F,p,'OLS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Select proper variable from x2-x6 using backward method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. 0 ['x2', 'x3', 'x4', 'x5', 'x6']\n",
      "No. 1 ['x2', 'x3', 'x4', 'x6']\n",
      "\n",
      "regression result\n",
      "\n",
      "method: backward\n",
      "\n",
      "-----------------\n",
      "\n",
      "y | beta | t | F_i | p(F)\n",
      "\n",
      "cons_ | 6007.32 | 2.68 | 0.00 | 0.00\n",
      "x2 | 5.07 | 3.73 | 13.89 | 0.00\n",
      "x3 | 2.31 | 4.75 | 22.56 | 0.00\n",
      "x4 | -824.26 | -4.91 | 24.14 | 0.00\n",
      "x6 | -862.70 | -3.71 | 13.77 | 0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.backward_reg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "Select proper variable from x2-x6 using gradually selecting method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. 0 []\n",
      "No. 1 ['x6']\n",
      "No. 2 []\n",
      "No. 3 ['x5']\n",
      "No. 4 []\n",
      "No. 5 ['x4']\n",
      "No. 6 []\n",
      "No. 7 ['x3']\n",
      "No. 8 ['x3', 'x4']\n",
      "No. 9 ['x3']\n",
      "No. 10 ['x3', 'x5']\n",
      "No. 11 ['x3', 'x5', 'x4']\n",
      "No. 12 ['x3', 'x5', 'x4', 'x6']\n",
      "No. 13 ['x3', 'x5', 'x4']\n",
      "No. 14 ['x3', 'x5', 'x4', 'x2']\n",
      "No. 15 ['x3', 'x5', 'x4']\n",
      "\n",
      "regression result\n",
      "\n",
      "method: gradual\n",
      "\n",
      "-----------------\n",
      "\n",
      "y | beta | t | F_i | p(F)\n",
      "\n",
      "cons_ | 1412.81 | 0.76 | 0.00 | 0.00\n",
      "x3 | 3.44 | 4.40 | 19.34 | 0.00\n",
      "x5 | 348.73 | 3.78 | 14.30 | 0.00\n",
      "x4 | -415.14 | -2.45 | 6.02 | 0.03\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.gradual_reg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    "Analyse the difference on above methods.\n",
    "\n",
    "Different method take different precoss to select variable, but they have a different preference on the number of final model variable.\n",
    "\n",
    "As for the gradual method, Most important varible will be taken to the model first, and there seems to be more competition between highly-relevant variable, so it is a little bit like winner takes all. In gerenal, it may lose some information of intermediate variable. In this content, x2,x3,x6 may have similiar increasing trend, once the x3 is selected, x2,x6 will have less chance to be selected.\n",
    "\n",
    "As for the backward method, it build full model at first, so contribution of each in the full environment seems to be more reasonable. And its model have a preference to take in more variable. The disadvantages of it is that it may take more redundant info and cause a wide interval of the estimates. In this context, it is more reasonable and explaintory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
